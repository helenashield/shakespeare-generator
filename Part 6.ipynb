{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Softmax, Dense, UpSampling1D, Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start out by opening up our poems file and doing some preprocessing to clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fairest creatures we desire increase,\n",
      "that thereby beauty's rose might never die,\n",
      "but as the riper should by time decease,\n",
      "his tender heir might bear his memory:\n",
      "but thou contracted to thine own bright eyes,\n",
      "feed'st thy light's flame with self-substantial fuel,\n",
      "making a famine where abundance lies,\n",
      "thy self thy foe, to thy sweet self too cruel:\n",
      "thou that art now the world's fresh ornament,\n",
      "and only herald to the gaudy spring,\n",
      "within thine own bud buriest thy content,\n",
      "and tender churl mak'st waste in niggarding:\n",
      "pity the world, or else this glutton be,\n",
      "to eat the world's due, by the grave and thee.\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "poems = list()\n",
    "\n",
    "with open(\"data/shakespeare.txt\") as f:\n",
    "    # The poem we are currently on\n",
    "    index = 0\n",
    "\n",
    "    # whether we are starting a new poem based on a line\n",
    "    new = False\n",
    "    for line in f:\n",
    "        # Remove leading spaces and the terminal new-line character\n",
    "        l = line.lstrip().lower()\n",
    "        \n",
    "        # If our line is not blank (blanks mean we are in between poems)\n",
    "        if(len(list(filter(None, l.split(\" \"))))>0):\n",
    "           \n",
    "            # Check if we already started a new poem\n",
    "            if(new):\n",
    "                # Add the line to the current poem\n",
    "                poems[-1]+=l\n",
    "                \n",
    "            # If we have not started, a new poem, start a new poem\n",
    "            else:\n",
    "                poems.append('')\n",
    "                new = True\n",
    "\n",
    "        # If we are on a blank, move to the next poem and indicate that we have not started a new one\n",
    "        else:\n",
    "            index += 1\n",
    "            new = False\n",
    "\n",
    "print(poems[0], end='')\n",
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = list()\n",
    "input_size = 40\n",
    "\n",
    "for poem in poems:\n",
    "    i = input_size\n",
    "    while i < len(poem)-1:\n",
    "        inputs.append([poem[i-input_size:i], poem[i]])\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from fairest creatures we desire increas', 'e']\n",
      "['rom fairest creatures we desire increase', ',']\n",
      "['om fairest creatures we desire increase,', '\\n']\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0])\n",
    "print(inputs[1])\n",
    "print(inputs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87359"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o 0\n",
      "e 1\n",
      ". 2\n",
      "j 3\n",
      ", 4\n",
      "v 5\n",
      "z 6\n",
      "w 7\n",
      ") 8\n",
      "( 9\n",
      "p 10\n",
      "? 11\n",
      "t 12\n",
      "f 13\n",
      "l 14\n",
      "\n",
      " 15\n",
      "c 16\n",
      "u 17\n",
      "a 18\n",
      ": 19\n",
      "! 20\n",
      "b 21\n",
      "n 22\n",
      "' 23\n",
      "r 24\n",
      "y 25\n",
      "d 26\n",
      "  27\n",
      "x 28\n",
      "m 29\n",
      "q 30\n",
      "k 31\n",
      "h 32\n",
      "- 33\n",
      "g 34\n",
      "s 35\n",
      "; 36\n",
      "i 37\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "encoding = dict()\n",
    "uniques = list(set(''.join(poems)))\n",
    "\n",
    "for i, s in enumerate(uniques):\n",
    "    encoding[s] = i\n",
    "\n",
    "for k, v in encoding.items():\n",
    "    print(k, v)\n",
    "\n",
    "encoding_size = len(encoding)\n",
    "print(encoding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    out = np.zeros([len(encoding)])\n",
    "    out[encoding[s]] = 1\n",
    "    return out\n",
    "\n",
    "def batch_encode(line):\n",
    "    #out = np.zeros([encoding_size*len(line)])\n",
    "    #for i in range(len(line)):\n",
    "    #    out[(i*encoding_size)+encoding[line[i]]] = 1\n",
    "    out = list()\n",
    "    for i in line:\n",
    "        out.append(encode(i))\n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 1. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 1. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 1. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 1. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "X = list()\n",
    "y = list()\n",
    "\n",
    "for i in inputs:\n",
    "    X.append(batch_encode(i[0]))\n",
    "    y.append(encode(i[1]))\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 100)               55600     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 38)                3838      \n",
      "=================================================================\n",
      "Total params: 59,438\n",
      "Trainable params: 59,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(100, input_shape=(X[0].shape)))\n",
    "model.add(Dense(encoding_size, activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "87359/87359 [==============================] - 113s 1ms/step - loss: 1.3872 - acc: 0.5694\n",
      "Epoch 2/10\n",
      "87359/87359 [==============================] - 113s 1ms/step - loss: 1.3750 - acc: 0.5733\n",
      "Epoch 3/10\n",
      "87359/87359 [==============================] - 113s 1ms/step - loss: 1.3624 - acc: 0.5754\n",
      "Epoch 4/10\n",
      "87359/87359 [==============================] - 113s 1ms/step - loss: 1.3508 - acc: 0.5788\n",
      "Epoch 5/10\n",
      "87359/87359 [==============================] - 115s 1ms/step - loss: 1.3395 - acc: 0.5822\n",
      "Epoch 6/10\n",
      "87359/87359 [==============================] - 114s 1ms/step - loss: 1.3278 - acc: 0.5854\n",
      "Epoch 7/10\n",
      "87359/87359 [==============================] - 115s 1ms/step - loss: 1.3171 - acc: 0.5885\n",
      "Epoch 8/10\n",
      "87359/87359 [==============================] - 115s 1ms/step - loss: 1.3068 - acc: 0.5917\n",
      "Epoch 9/10\n",
      "87359/87359 [==============================] - 116s 1ms/step - loss: 1.2970 - acc: 0.5940\n",
      "Epoch 10/10\n",
      "87359/87359 [==============================] - 118s 1ms/step - loss: 1.2874 - acc: 0.5976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4fe0860>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our first model, lets write some functions to generate poems. We need a function which probabilistically selects an item from an array of percents, a function which generates the next character based on an input and a function which generates a poem given a model and a seed string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(ar):\n",
    "    i = 0\n",
    "    r = random.random()\n",
    "    while(np.sum(ar[0][:i+1])<r):\n",
    "        i+=1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(model, line):\n",
    "    return uniques[prob(model.predict(np.array(batch_encode(line)).reshape(1, len(line), encoding_size)))]\n",
    "\n",
    "def get_next_det(model, line):\n",
    "    return uniques[np.argmax(model.predict(np.array(batch_encode(line)).reshape(1, len(line), encoding_size)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = 'from fairest creatures we desire increas'\n",
    "get_next(model, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fairest creatures we desire increast.\n",
      "but siuguch things truthting thee were thine,\n",
      "when i live they shall sprindams my dead.\n",
      "lities volder tond, wist the pition, utward.\n",
      "their eprent of thes, but thee hamp's poor,\n",
      "the purpust thee good in buind unjubledd:\n",
      "my smused, and this geipl can thus a prease.\n",
      "love that upon his is gives this shim:\n",
      "how which is you mow, which must assures from love bud,\n",
      "which he stoe wond ride to heart which it not knows,\n",
      "whith when those by and carrectares are now,\n",
      "while is before of the frefound, i dwelled,\n",
      "is sequent that i quinds so days in dryst.\n",
      "the mory not i may a hisefowned cond.\n",
      "\n",
      "from fairest creatures we desire increase,\n",
      "which i desprice of the time that thou shoughtsome doth state the sun a see thee,\n",
      "and so thou art thou art is i not to age,\n",
      "to the time that thou art to the store dead,\n",
      "that i have such a too and the world with those,\n",
      "and thou art to the store to the world,\n",
      "then thand me to the sun a see to my see,\n",
      "so the sun a self all tooked thee,\n",
      "and do summer's dead with thy self thy self,\n",
      "i death the time that thou art thou forged,\n",
      "my self in thee as a self all the sun,\n",
      "comment to the world i have strange stole,\n",
      "which is the sun a shore to the time despise,\n",
      "which i desprice of the time that thou shoughtsome doth state the sun a see thee,\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"from fairest creatures we desire increase,\\nwhich i desprice of the time that thou shoughtsome doth state the sun a see thee,\\nand so thou art thou art is i not to age,\\nto the time that thou art to the store dead,\\nthat i have such a too and the world with those,\\nand thou art to the store to the world,\\nthen thand me to the sun a see to my see,\\nso the sun a self all tooked thee,\\nand do summer's dead with thy self thy self,\\ni death the time that thou art thou forged,\\nmy self in thee as a self all the sun,\\ncomment to the world i have strange stole,\\nwhich is the sun a shore to the time despise,\\nwhich i desprice of the time that thou shoughtsome doth state the sun a see thee,\\n\""
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_poem(model, seed='from fairest creatures we desire increas'):\n",
    "    out = seed\n",
    "    l = 0\n",
    "    while l < 14:\n",
    "        new = get_next(model, out[-len(seed):])\n",
    "        if(new == '\\n'):\n",
    "            l+=1\n",
    "        out+=new\n",
    "    print(out)\n",
    "    return out\n",
    "\n",
    "gen_poem(model)\n",
    "\n",
    "def gen_det_poem(model, seed='from fairest creatures we desire increas'):\n",
    "    out = seed\n",
    "    l = 0\n",
    "    while l < 14:\n",
    "        new = get_next_det(model, out[-len(seed):])\n",
    "        if(new == '\\n'):\n",
    "            l+=1\n",
    "        out+=new\n",
    "    print(out)\n",
    "    return out\n",
    "\n",
    "gen_det_poem(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that is a good starting point, but we could do better. Lets first see how the number of LSTM cells affects the overall performance. We will train 100, 200 and 150 cell LSTMs, and then see if different optimizers yield different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_15 (LSTM)               (None, 100)               55600     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 38)                3838      \n",
      "=================================================================\n",
      "Total params: 59,438\n",
      "Trainable params: 59,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "87359/87359 [==============================] - 137s 2ms/step - loss: 2.4379 - acc: 0.3018\n",
      "Epoch 2/50\n",
      "87359/87359 [==============================] - 117s 1ms/step - loss: 2.0778 - acc: 0.3883\n",
      "Epoch 3/50\n",
      "87359/87359 [==============================] - 117s 1ms/step - loss: 1.9392 - acc: 0.4202\n",
      "Epoch 4/50\n",
      "87359/87359 [==============================] - 122s 1ms/step - loss: 1.8498 - acc: 0.4465\n",
      "Epoch 5/50\n",
      "87359/87359 [==============================] - 118s 1ms/step - loss: 1.7913 - acc: 0.4618\n",
      "Epoch 6/50\n",
      "87359/87359 [==============================] - 112s 1ms/step - loss: 1.7446 - acc: 0.4736\n",
      "Epoch 7/50\n",
      "87359/87359 [==============================] - 114s 1ms/step - loss: 1.7052 - acc: 0.4826\n",
      "Epoch 8/50\n",
      "87359/87359 [==============================] - 113s 1ms/step - loss: 1.6732 - acc: 0.4909\n",
      "Epoch 9/50\n",
      "87359/87359 [==============================] - 114s 1ms/step - loss: 1.6450 - acc: 0.4974\n",
      "Epoch 10/50\n",
      "87359/87359 [==============================] - 115s 1ms/step - loss: 1.6184 - acc: 0.5048\n",
      "Epoch 11/50\n",
      "87359/87359 [==============================] - 115s 1ms/step - loss: 1.5953 - acc: 0.5107\n",
      "Epoch 12/50\n",
      "87359/87359 [==============================] - 118s 1ms/step - loss: 1.5731 - acc: 0.5174\n",
      "Epoch 13/50\n",
      "87359/87359 [==============================] - 117s 1ms/step - loss: 1.5538 - acc: 0.5222\n",
      "Epoch 14/50\n",
      "87359/87359 [==============================] - 117s 1ms/step - loss: 1.5359 - acc: 0.5270\n",
      "Epoch 15/50\n",
      "87359/87359 [==============================] - 118s 1ms/step - loss: 1.5189 - acc: 0.5303\n",
      "Epoch 16/50\n",
      "87359/87359 [==============================] - 111s 1ms/step - loss: 1.5033 - acc: 0.5355\n",
      "Epoch 17/50\n",
      "87359/87359 [==============================] - 110s 1ms/step - loss: 1.4888 - acc: 0.5405\n",
      "Epoch 18/50\n",
      "87359/87359 [==============================] - 111s 1ms/step - loss: 1.4747 - acc: 0.5438\n",
      "Epoch 19/50\n",
      "87359/87359 [==============================] - 109s 1ms/step - loss: 1.4610 - acc: 0.5481\n",
      "Epoch 20/50\n",
      "87359/87359 [==============================] - 110s 1ms/step - loss: 1.4479 - acc: 0.5520\n",
      "Epoch 21/50\n",
      "87359/87359 [==============================] - 1931s 22ms/step - loss: 1.4363 - acc: 0.5539\n",
      "Epoch 22/50\n",
      "87359/87359 [==============================] - 13249s 152ms/step - loss: 1.4237 - acc: 0.5566\n",
      "Epoch 23/50\n",
      "87359/87359 [==============================] - 5282s 60ms/step - loss: 1.4135 - acc: 0.5600\n",
      "Epoch 24/50\n",
      "87359/87359 [==============================] - 143s 2ms/step - loss: 1.4029 - acc: 0.5642\n",
      "Epoch 25/50\n",
      "87359/87359 [==============================] - 135s 2ms/step - loss: 1.3917 - acc: 0.5681\n",
      "Epoch 26/50\n",
      "87359/87359 [==============================] - 118s 1ms/step - loss: 1.3805 - acc: 0.5694\n",
      "Epoch 27/50\n",
      "87359/87359 [==============================] - 139s 2ms/step - loss: 1.3716 - acc: 0.5729\n",
      "Epoch 28/50\n",
      "87359/87359 [==============================] - 146s 2ms/step - loss: 1.3603 - acc: 0.5768\n",
      "Epoch 29/50\n",
      "87359/87359 [==============================] - 148s 2ms/step - loss: 1.3510 - acc: 0.5788\n",
      "Epoch 30/50\n",
      "87359/87359 [==============================] - 150s 2ms/step - loss: 1.3417 - acc: 0.5816\n",
      "Epoch 31/50\n",
      "87359/87359 [==============================] - 152s 2ms/step - loss: 1.3320 - acc: 0.5848\n",
      "Epoch 32/50\n",
      "87359/87359 [==============================] - 153s 2ms/step - loss: 1.3223 - acc: 0.5874\n",
      "Epoch 33/50\n",
      "87359/87359 [==============================] - 153s 2ms/step - loss: 1.3146 - acc: 0.5893\n",
      "Epoch 34/50\n",
      "87359/87359 [==============================] - 153s 2ms/step - loss: 1.3051 - acc: 0.5924\n",
      "Epoch 35/50\n",
      "87359/87359 [==============================] - 152s 2ms/step - loss: 1.2963 - acc: 0.5929\n",
      "Epoch 36/50\n",
      "87359/87359 [==============================] - 118s 1ms/step - loss: 1.2880 - acc: 0.5977\n",
      "Epoch 37/50\n",
      "87359/87359 [==============================] - 115s 1ms/step - loss: 1.2798 - acc: 0.5993\n",
      "Epoch 38/50\n",
      "87359/87359 [==============================] - 119s 1ms/step - loss: 1.2736 - acc: 0.6009\n",
      "Epoch 39/50\n",
      "87359/87359 [==============================] - 126s 1ms/step - loss: 1.2651 - acc: 0.6036\n",
      "Epoch 40/50\n",
      "87359/87359 [==============================] - 118s 1ms/step - loss: 1.2588 - acc: 0.6053\n",
      "Epoch 41/50\n",
      "87359/87359 [==============================] - 139s 2ms/step - loss: 1.2505 - acc: 0.6084\n",
      "Epoch 42/50\n",
      "87359/87359 [==============================] - 141s 2ms/step - loss: 1.2445 - acc: 0.6101\n",
      "Epoch 43/50\n",
      "87359/87359 [==============================] - 126s 1ms/step - loss: 1.2371 - acc: 0.6125\n",
      "Epoch 44/50\n",
      "87359/87359 [==============================] - 129s 1ms/step - loss: 1.2312 - acc: 0.6123\n",
      "Epoch 45/50\n",
      "87359/87359 [==============================] - 133s 2ms/step - loss: 1.2237 - acc: 0.6161\n",
      "Epoch 46/50\n",
      "87359/87359 [==============================] - 134s 2ms/step - loss: 1.2168 - acc: 0.6180 1s - lo\n",
      "Epoch 47/50\n",
      "87359/87359 [==============================] - 114s 1ms/step - loss: 1.2121 - acc: 0.6177\n",
      "Epoch 48/50\n",
      "87359/87359 [==============================] - 113s 1ms/step - loss: 1.2062 - acc: 0.6211\n",
      "Epoch 49/50\n",
      "87359/87359 [==============================] - 112s 1ms/step - loss: 1.2003 - acc: 0.6218\n",
      "Epoch 50/50\n",
      "87359/87359 [==============================] - 114s 1ms/step - loss: 1.1962 - acc: 0.6227\n",
      "from fairest creatures we desire increase,\n",
      "hading eyes more blesseds may pleasure fare,\n",
      "somein it heaven's pensure hamp so place\n",
      "the weak museal torse a peremity behold:\n",
      "thou to ruterted gartunely come to wind.\n",
      "jue siving own canse confored a do poorn,\n",
      "and that come how needs befullin-beauty doan,\n",
      "and thou livis come to the firety disdess home,\n",
      "which every nothing thought, having sovers doth,\n",
      "best are whichs past confol pentered fingloon,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(LSTM(100, input_shape=(X[0].shape)))\n",
    "model1.add(Dense(encoding_size, activation='softmax'))\n",
    "\n",
    "print(model1.summary())\n",
    "\n",
    "# compile model\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "model1.fit(X, y, epochs=50, verbose=1)\n",
    "\n",
    "output1 = 'from fairest creatures we desire increas'\n",
    "\n",
    "l = 0\n",
    "while l < 10:\n",
    "    new = get_next(output1[-40:])\n",
    "    if(new == '\\n'):\n",
    "        l+=1\n",
    "    output1+=new\n",
    "print(output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I tried to run this overnight, but my computer did not want to. Started again from here, reducing the epochs to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_17 (LSTM)               (None, 200)               191200    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 38)                7638      \n",
      "=================================================================\n",
      "Total params: 198,838\n",
      "Trainable params: 198,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "87359/87359 [==============================] - 173s 2ms/step - loss: 2.2801 - acc: 0.3404\n",
      "Epoch 2/10\n",
      "87359/87359 [==============================] - 182s 2ms/step - loss: 1.8673 - acc: 0.4414\n",
      "Epoch 3/10\n",
      "87359/87359 [==============================] - 186s 2ms/step - loss: 1.7194 - acc: 0.4768\n",
      "Epoch 4/10\n",
      "87359/87359 [==============================] - 188s 2ms/step - loss: 1.6204 - acc: 0.5028\n",
      "Epoch 5/10\n",
      "87359/87359 [==============================] - 185s 2ms/step - loss: 1.5418 - acc: 0.5244\n",
      "Epoch 6/10\n",
      "87359/87359 [==============================] - 196s 2ms/step - loss: 1.4718 - acc: 0.5431\n",
      "Epoch 7/10\n",
      "87359/87359 [==============================] - 181s 2ms/step - loss: 1.4092 - acc: 0.5588\n",
      "Epoch 8/10\n",
      "87359/87359 [==============================] - 189s 2ms/step - loss: 1.3480 - acc: 0.5751\n",
      "Epoch 9/10\n",
      "87359/87359 [==============================] - 174s 2ms/step - loss: 1.2898 - acc: 0.5922\n",
      "Epoch 10/10\n",
      "87359/87359 [==============================] - 172s 2ms/step - loss: 1.2320 - acc: 0.6109\n",
      "from fairest creatures we desire increase\n",
      "which conter,\n",
      "yet if fould fissly, to husked no lore:\n",
      "for staubles not to thou thy segared was,\n",
      "ot child) dooned face as geasunge my const,\n",
      "you they with my fair, and must rewing'st should canceldse decemanceled,\n",
      "this spirit in dead when (desight done:\n",
      "my love to to the despicted to woe,\n",
      "and dasked wouch with your phanss wild stone,\n",
      "provincess comom be ack) 'tis heaven to love?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(LSTM(200, input_shape=(X[0].shape)))\n",
    "model2.add(Dense(encoding_size, activation='softmax'))\n",
    "\n",
    "print(model2.summary())\n",
    "\n",
    "# compile model\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "model2.fit(X, y, epochs=10, verbose=1)\n",
    "\n",
    "output2 = 'from fairest creatures we desire increas'\n",
    "\n",
    "l = 0\n",
    "while l < 10:\n",
    "    new = get_next(output2[-40:])\n",
    "    if(new == '\\n'):\n",
    "        l+=1\n",
    "    output2+=new\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_18 (LSTM)               (None, 150)               113400    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 38)                5738      \n",
      "=================================================================\n",
      "Total params: 119,138\n",
      "Trainable params: 119,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "87359/87359 [==============================] - 148s 2ms/step - loss: 2.3717 - acc: 0.3219\n",
      "Epoch 2/10\n",
      "87359/87359 [==============================] - 143s 2ms/step - loss: 1.9772 - acc: 0.4099\n",
      "Epoch 3/10\n",
      "87359/87359 [==============================] - 146s 2ms/step - loss: 1.8189 - acc: 0.4550\n",
      "Epoch 4/10\n",
      "87359/87359 [==============================] - 150s 2ms/step - loss: 1.7234 - acc: 0.4788\n",
      "Epoch 5/10\n",
      "87359/87359 [==============================] - 150s 2ms/step - loss: 1.6532 - acc: 0.4957\n",
      "Epoch 6/10\n",
      "87359/87359 [==============================] - 144s 2ms/step - loss: 1.5957 - acc: 0.5106\n",
      "Epoch 7/10\n",
      "87359/87359 [==============================] - 149s 2ms/step - loss: 1.5467 - acc: 0.5252\n",
      "Epoch 8/10\n",
      "87359/87359 [==============================] - 151s 2ms/step - loss: 1.5036 - acc: 0.5361\n",
      "Epoch 9/10\n",
      "87359/87359 [==============================] - 159s 2ms/step - loss: 1.4634 - acc: 0.5480\n",
      "Epoch 10/10\n",
      "87359/87359 [==============================] - 143s 2ms/step - loss: 1.4262 - acc: 0.5574\n",
      "from fairest creatures we desire increase,\n",
      "life hought made my verse i not that plafued.\n",
      "for still apon thy rave slaved in on,\n",
      "save i many and plaised with edreds,\n",
      "to ever sumour verte pacconed, by new.\n",
      "but was termed i with hes in eye bich,\n",
      "frembs of a man mone, and touches) to pars.\n",
      "to forbuch compound) that that thy behold,\n",
      "a mone her oncelffle swalgnet, mark woold.\n",
      "then be peweminuch is his stol'st thou gaven.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model3 = Sequential()\n",
    "\n",
    "model3.add(LSTM(150, input_shape=(X[0].shape)))\n",
    "model3.add(Dense(encoding_size, activation='softmax'))\n",
    "\n",
    "print(model3.summary())\n",
    "\n",
    "# compile model\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "model3.fit(X, y, epochs=10, verbose=1)\n",
    "\n",
    "output3 = 'from fairest creatures we desire increas'\n",
    "\n",
    "l = 0\n",
    "while l < 10:\n",
    "    new = get_next(output3[-40:])\n",
    "    if(new == '\\n'):\n",
    "        l+=1\n",
    "    output3+=new\n",
    "print(output3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I realized here that the poems I was outputting were based on the original model. The code below was modified to generate poems using the relevant model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_21 (LSTM)               (None, 100)               55600     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 38)                3838      \n",
      "=================================================================\n",
      "Total params: 59,438\n",
      "Trainable params: 59,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "87359/87359 [==============================] - 137s 2ms/step - loss: 2.4507 - acc: 0.2992\n",
      "Epoch 2/10\n",
      "87359/87359 [==============================] - 121s 1ms/step - loss: 2.0729 - acc: 0.3914\n",
      "Epoch 3/10\n",
      "87359/87359 [==============================] - 117s 1ms/step - loss: 1.9279 - acc: 0.4300\n",
      "Epoch 4/10\n",
      "87359/87359 [==============================] - 118s 1ms/step - loss: 1.8392 - acc: 0.4526\n",
      "Epoch 5/10\n",
      "87359/87359 [==============================] - 119s 1ms/step - loss: 1.7779 - acc: 0.4673\n",
      "Epoch 6/10\n",
      "87359/87359 [==============================] - 117s 1ms/step - loss: 1.7287 - acc: 0.4801\n",
      "Epoch 7/10\n",
      "87359/87359 [==============================] - 118s 1ms/step - loss: 1.6879 - acc: 0.4890\n",
      "Epoch 8/10\n",
      "87359/87359 [==============================] - 119s 1ms/step - loss: 1.6542 - acc: 0.4999\n",
      "Epoch 9/10\n",
      "87359/87359 [==============================] - 120s 1ms/step - loss: 1.6240 - acc: 0.5065\n",
      "Epoch 10/10\n",
      "87359/87359 [==============================] - 122s 1ms/step - loss: 1.5982 - acc: 0.5147\n",
      "from fairest creatures we desire increased:\n",
      "when i henon, faity burusted beriese vis,\n",
      "enseet shifter wounds have hacosund whindy guch dreash,\n",
      "andseare with his from my sightsing true,\n",
      "whicc s me thee are and ellougond shourd,\n",
      "that that my see wrift, as telfe to prease,\n",
      "whan i lon stall my right us farsis and ara.\n",
      "waimed nerd, and my beart newsling bead doon).\n",
      "in fellors were oundint or athor be,\n",
      "thy seemars my becose thou art ads their wat.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model4 = Sequential()\n",
    "\n",
    "model4.add(LSTM(100, input_shape=(X[0].shape)))\n",
    "model4.add(Dense(encoding_size, activation='softmax'))\n",
    "\n",
    "print(model4.summary())\n",
    "\n",
    "# compile model\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "model4.fit(X, y, epochs=10, verbose=1)\n",
    "\n",
    "p4_1 = gen_poem(model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_22 (LSTM)               (None, 100)               55600     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 38)                3838      \n",
      "=================================================================\n",
      "Total params: 59,438\n",
      "Trainable params: 59,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "87359/87359 [==============================] - 113s 1ms/step - loss: 2.3943 - acc: 0.3146\n",
      "Epoch 2/10\n",
      "87359/87359 [==============================] - 111s 1ms/step - loss: 1.9885 - acc: 0.4137\n",
      "Epoch 3/10\n",
      "87359/87359 [==============================] - 112s 1ms/step - loss: 1.8509 - acc: 0.4516\n",
      "Epoch 4/10\n",
      "87359/87359 [==============================] - 112s 1ms/step - loss: 1.7680 - acc: 0.4732\n",
      "Epoch 5/10\n",
      "87359/87359 [==============================] - 112s 1ms/step - loss: 1.7068 - acc: 0.4893\n",
      "Epoch 6/10\n",
      "87359/87359 [==============================] - 111s 1ms/step - loss: 1.6602 - acc: 0.5018\n",
      "Epoch 7/10\n",
      "87359/87359 [==============================] - 115s 1ms/step - loss: 1.6192 - acc: 0.5133\n",
      "Epoch 8/10\n",
      "87359/87359 [==============================] - 113s 1ms/step - loss: 1.5867 - acc: 0.5214\n",
      "Epoch 9/10\n",
      "87359/87359 [==============================] - 112s 1ms/step - loss: 1.5556 - acc: 0.5305\n",
      "Epoch 10/10\n",
      "87359/87359 [==============================] - 112s 1ms/step - loss: 1.5299 - acc: 0.5376\n",
      "from fairest creatures we desire increases be,\n",
      "s)aw with thy sweet sight is part, to thee,\n",
      "so pifit, thy soush of which sight uh amd,\n",
      "i his onf\n",
      "raise this a correout in dalscameig wwith,\n",
      "who hoven menets ly tousure his my deired,\n",
      "ruckents that or were now when in your,\n",
      "which hyse forth chinds truth yourmadd no han,\n",
      "no light not dif commersion be seem\n",
      "prace best ver my age ithing so nog thence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model5 = Sequential()\n",
    "\n",
    "model5.add(LSTM(100, input_shape=(X[0].shape)))\n",
    "model5.add(Dense(encoding_size, activation='softmax'))\n",
    "\n",
    "print(model5.summary())\n",
    "\n",
    "# compile model\n",
    "model5.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "model5.fit(X, y, epochs=10, verbose=1)\n",
    "\n",
    "p5_1 = gen_poem(model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model 1\n",
      "from fairest creatures we desire increased\n",
      "biuth po that thou, or tells not reteated,\n",
      "but of losous suffeit of thy dead, or true deneess,\n",
      "and view by ser it by night, or new truth,\n",
      "and of their motting sive whether rends ancquence:\n",
      "make love that thou full outamining thy macced,\n",
      "it should confounding to be reliving,\n",
      "and praise they did vances cunk their baro's sight,\n",
      "i canf erityling wrace gives and grached\n",
      "than hid love thrate the old a soeft remiew.\n",
      "\n",
      "\n",
      " Model 2\n",
      "from fairest creatures we desire increase,\n",
      "nor state with sun, in your line of his where.\n",
      "in my feering should to diew and butt.\n",
      "and sut that beauty pright the flame you gree,\n",
      "of thy tomminess to chose onr thing eorer.\n",
      "un all-every, bright thy boutts hath her blessed,\n",
      "yet your proud might vade had auduse one,\n",
      "your foce thou my should and kenters wored.\n",
      "your proud life's i thou art fearst,\n",
      "but fleating of more tall by antower's eye,\n",
      "\n",
      "\n",
      " Model 3\n",
      "from fairest creatures we desire increase\n",
      "thou laiked will stord to self absent tanght,\n",
      "i thy beauty live thy gifes of the happ\n",
      "as i appaster pleasure watin she had,\n",
      "yet i in my things of thy my, arron,\n",
      "and summer's more jurrechedied eyes ints dfone\n",
      "mine child that xlow keep anbaited love so alone.\n",
      "more life thy pain the things the from an'sing,\n",
      "when illool one mefored and phoss cold\n",
      "and make see her things thy deepill all make:\n",
      "\n",
      "\n",
      " Model 4\n",
      "from fairest creatures we desire increase,\n",
      "and me lemost bealth:\n",
      "and true, thy prives in me your thee wrow,\n",
      "though tine eye istait acoust time,\n",
      "the loss right, and my ore, to groenvay:\n",
      "the calains cortancough ny tongure?\n",
      "as padce sum must can ve pise on i mo trew,\n",
      "afany he there hath hath intaygh, ster(tedming liesp.\n",
      "to sto conrsagive, and the rondense tom,\n",
      "me wery bure, so it uncemmers one sout.ling.\n",
      "\n",
      "\n",
      " Model 5\n",
      "from fairest creatures we desire increased,\n",
      "seeleds' hot faineg to his garns of i nowe,\n",
      "so she numpends vict let with is his a geathe\n",
      "\n",
      "naulthey sace before tam ndre themering dooln\n",
      "tinding one sa, you it men till i fri,wy.\n",
      "for thy godean unwhese that me she to hade,\n",
      "and not wilt moor say my leave to state\n",
      "how fair i fain eyely sight nover donchor.\n",
      "bind, my betond be than sweach your selforeb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n Model 1')\n",
    "p1_1 = gen_poem(model1)\n",
    "print('\\n Model 2')\n",
    "p2_1 = gen_poem(model2)\n",
    "print('\\n Model 3')\n",
    "p3_1 = gen_poem(model3)\n",
    "print('\\n Model 4')\n",
    "p4_1 = gen_poem(model4)\n",
    "print('\\n Model 5')\n",
    "p5_1 = gen_poem(model5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the 200 cell LSTM looks best, based on the number of epochs run. It looks almost equivalent to model 1, with only 1/5th of the epochs. However, lets try again with a deterministic approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model 1\n",
      "from fairest creatures we desire increase,\n",
      "but thou art but find the self thou arting,\n",
      "and the cure a coutit hast i see the deeds.\n",
      "then summer's filler so fair the world i will grow,\n",
      "that i am all the true dear state eyes still,\n",
      "thou wilt thou art thou art but there i chane\n",
      "or all the world be the true dear prove,\n",
      "to show i courses of the true dear heart\n",
      "to make me but what i will be thou art still,\n",
      "to show i that i am all the black age,\n",
      "\n",
      "\n",
      " Model 2\n",
      "from fairest creatures we desire increase,\n",
      "the sure with thy self thou wilt love thee,\n",
      "and therefore that which i have spend, where you dost strievis,\n",
      "the world the sweet seeming that with thee,\n",
      "and therefore that which i have spend, where you dost strievis,\n",
      "the world the sweet seeming that with thee,\n",
      "and therefore that which i have spend, where you dost strievis,\n",
      "the world the sweet seeming that with thee,\n",
      "and therefore that which i have spend, where you dost strievis,\n",
      "the world the sweet seeming that with thee,\n",
      "\n",
      "\n",
      " Model 3\n",
      "from fairest creatures we desire increase,\n",
      "when i should the summer's shall the strong,\n",
      "the beauty shall the thing the beauty show,\n",
      "the beauty of the self are so summer,\n",
      "when i should the summer's shall the strong,\n",
      "the beauty shall the thing the beauty show,\n",
      "the beauty of the self are so summer,\n",
      "when i should the summer's shall the strong,\n",
      "the beauty shall the thing the beauty show,\n",
      "the beauty of the self are so summer,\n",
      "\n",
      "\n",
      " Model 4\n",
      "from fairest creatures we desire increase,\n",
      "and the see in the see the see the wore,\n",
      "the worth the see the see the see the wore,\n",
      "the worth the see the see the see the wore,\n",
      "the worth the see the see the see the wore,\n",
      "the worth the see the see the see the wore,\n",
      "the worth the see the see the see the wore,\n",
      "the worth the see the see the see the wore,\n",
      "the worth the see the see the see the wore,\n",
      "the worth the see the see the see the wore,\n",
      "\n",
      "\n",
      " Model 5\n",
      "from fairest creatures we desire increase,\n",
      "then in the forth of thee that thou art,\n",
      "and thou art thou art the world to be thee,\n",
      "then beauty shall the world to be thee more,\n",
      "and thou art thou art the world to be thee,\n",
      "then beauty shall the world to be thee more,\n",
      "and thou art thou art the world to be thee,\n",
      "then beauty shall the world to be thee more,\n",
      "and thou art thou art the world to be thee,\n",
      "then beauty shall the world to be thee more,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n Model 1')\n",
    "p1_2 = gen_det_poem(model1)\n",
    "print('\\n Model 2')\n",
    "p2_2 = gen_det_poem(model2)\n",
    "print('\\n Model 3')\n",
    "p3_2 = gen_det_poem(model3)\n",
    "print('\\n Model 4')\n",
    "p4_2 = gen_det_poem(model4)\n",
    "print('\\n Model 5')\n",
    "p5_2 = gen_det_poem(model5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like deterministic models require more training to generate non-repeating poems. We will generate a few more poems with models 2 and 3, just to compare a little more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model 2\n",
      "from fairest creatures we desire increased.\n",
      "thou art argament of thine ever tell.\n",
      "for far well that beauty my lov's mace curis, where all know\n",
      "whe haven my score thus sweet spy mine eyes,\n",
      "and new to she in and so, thine one flest,\n",
      "and praise made oden of excuse with thine enguch.\n",
      "for night my finder, (ald muster from the ftime\n",
      "which spoance art night shall gives alant.\n",
      "un, mad thou shouldst for mourd, by addity,\n",
      "which lies as heart it sade) you htan no, need,\n",
      "\n",
      "from fairest creatures we desire increasake:\n",
      "come fort youb, your false orn thee sabs coousence\n",
      "the gaudy will loving bring deep summ,\n",
      "arove that vory are pastirg strands berought:\n",
      "bur vide i with thy sack and forbest teeds.\n",
      "there, out thy sweet words mor tyrmect gonged,\n",
      "duess not to grow) and he thought, without reseather spont.\n",
      "and which i steal hoth, from thy nurge disgrace:\n",
      "which tanthy sid, frack why some inater bast)\n",
      "showing thy friends with murion ender:\n",
      "\n",
      "from fairest creatures we desire increase,\n",
      "why minter thange sees not desevirs, when a part,\n",
      "even so butivies thee time's dear stalourbed,\n",
      "my dofe cannes that with winty outhse store,\n",
      "of thee aland's thou mightst stave to mate,\n",
      "which those brough dwall, and where though vice slailing hud\n",
      "which beghalted jeceavid to should looks in grent\n",
      "as youth have ever with thy how, withis argnace\n",
      "day from their ring and mean's hath his mend,\n",
      "and trot thy should dut in due my deeds,\n",
      "\n",
      "\n",
      " Model 3\n",
      "from fairest creatures we desire increass,\n",
      "by where's i (lose-theys' age streach shall be from nome.\n",
      "which prousen to sormort like woyed me to be,\n",
      "my stumpie i swores rablouge as old mind)\n",
      "the baiquladies of nearing that die dising,\n",
      "haw with fal here praise tcome well me bene\n",
      "with theirirs a sweet friend fainoly worsh\n",
      "of some sive's excily' thee wheretimes days,\n",
      "and yeur time the thingn by upper, leave in presert,\n",
      "nor doth that my beauty? which ewrite evef yout;\n",
      "\n",
      "from fairest creatures we desire increase.\n",
      "but my heart thou my love as fapmise nots,\n",
      "that as thy self mine gond, the vaires should night sorromy with\n",
      "ankernabow, she indoming to hiss,\n",
      "it added in thinces wherethough of the blad\n",
      "estoce easthing that were with praise the away,\n",
      "nor dear lend of with spunty baint\n",
      "how in well me those the bforce, and thee is i winds,\n",
      "i dead near such summity desertion,\n",
      "from their ithat the summerion of adgett.\n",
      "\n",
      "from fairest creatures we desire increase,\n",
      "which of this, and making with sigh fort.\n",
      "yet laid is trou the perered owd'st butthot mine ere;\n",
      "outhty the toll own repuinter eyes with their die.\n",
      "rach thy sweet pleas to hath my bemond.\n",
      "there infence self deppaseses your fase,\n",
      "remeaped they by thy beauty's eye the look,\n",
      "nor by numbered the reatthougal kies to praise,\n",
      "but by the those is my thine corthanden's scap,\n",
      "which i am fal h's dade out shouldst infease!,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n Model 2')\n",
    "p2_3 = gen_poem(model2)\n",
    "p2_4 = gen_poem(model2)\n",
    "p2_5 = gen_poem(model2)\n",
    "print('\\n Model 3')\n",
    "p3_3 = gen_poem(model3)\n",
    "p3_4 = gen_poem(model3)\n",
    "p3_5 = gen_poem(model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So these are proving too difficult to compare. I will go with the 2nd model, simply because it reached a higher training accuracy (0.61 vs 0.56). We will train the same model for an extra 15 epochs, see how it is doing, and then train for another 25 if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "87359/87359 [==============================] - 168s 2ms/step - loss: 1.1755 - acc: 0.6279\n",
      "Epoch 2/15\n",
      "87359/87359 [==============================] - 171s 2ms/step - loss: 1.1223 - acc: 0.6437\n",
      "Epoch 3/15\n",
      "87359/87359 [==============================] - 171s 2ms/step - loss: 1.0742 - acc: 0.6579\n",
      "Epoch 4/15\n",
      "87359/87359 [==============================] - 175s 2ms/step - loss: 1.0298 - acc: 0.6715\n",
      "Epoch 5/15\n",
      "87359/87359 [==============================] - 155s 2ms/step - loss: 0.9899 - acc: 0.6836\n",
      "Epoch 6/15\n",
      "87359/87359 [==============================] - 150s 2ms/step - loss: 0.9511 - acc: 0.6955\n",
      "Epoch 7/15\n",
      "87359/87359 [==============================] - 151s 2ms/step - loss: 0.9177 - acc: 0.7061\n",
      "Epoch 8/15\n",
      "87359/87359 [==============================] - 151s 2ms/step - loss: 0.8898 - acc: 0.7153\n",
      "Epoch 9/15\n",
      "87359/87359 [==============================] - 151s 2ms/step - loss: 0.8621 - acc: 0.7235\n",
      "Epoch 10/15\n",
      "87359/87359 [==============================] - 150s 2ms/step - loss: 0.8375 - acc: 0.7305\n",
      "Epoch 11/15\n",
      "87359/87359 [==============================] - 152s 2ms/step - loss: 0.8151 - acc: 0.7385\n",
      "Epoch 12/15\n",
      "87359/87359 [==============================] - 151s 2ms/step - loss: 0.7981 - acc: 0.7416\n",
      "Epoch 13/15\n",
      "87359/87359 [==============================] - 152s 2ms/step - loss: 0.7764 - acc: 0.7485\n",
      "Epoch 14/15\n",
      "87359/87359 [==============================] - 166s 2ms/step - loss: 0.7592 - acc: 0.7544\n",
      "Epoch 15/15\n",
      "87359/87359 [==============================] - 196s 2ms/step - loss: 0.7450 - acc: 0.7575\n",
      "from fairest creatures we desire increase,\n",
      "great uils your life no part of abond,\n",
      "but you best, nor like of all my love doth neg.\n",
      "rod in love, against my self i'll foot\n",
      "hours, and picture to sweet despressed sweets:\n",
      "but he life even to thee readon hige.\n",
      "i it a werangt the healt-so parthing warth.\n",
      "for helper to help on the lively spitious,\n",
      "and dim nimbeact them be with she mine away.\n",
      "authation for, and for their ragest.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "model2.fit(X, y, epochs=15, verbose=1)\n",
    "\n",
    "p2_6 = gen_poem(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fairest creatures we desire increasure,\n",
      "they do but sweets with sweets, and my desire,\n",
      "her from their preser assentle surmeand,\n",
      "that i am sor thee, thy worth the grous and made,\n",
      "but for his scythe come him on thee but bear,\n",
      "my love is ansed do none eyes,\n",
      "fair from her false-bandance is allowed,\n",
      "but yet my rebeliv's bentomed, are breeds,\n",
      "and nothing stands but in the state,\n",
      "when i against my self it donds,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p2_7 = gen_det_poem(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both probabilistic and deterministic look good, but more training would not hurt. I'm getting lunch now, so I'll train it for another 25, which should take 1:15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "87359/87359 [==============================] - 172s 2ms/step - loss: 0.7320 - acc: 0.7611\n",
      "Epoch 2/25\n",
      "87359/87359 [==============================] - 196s 2ms/step - loss: 0.7174 - acc: 0.7662\n",
      "Epoch 3/25\n",
      "87359/87359 [==============================] - 184s 2ms/step - loss: 0.7016 - acc: 0.7704 0s - loss: 0.7014 - ac\n",
      "Epoch 4/25\n",
      "87359/87359 [==============================] - 183s 2ms/step - loss: 0.6929 - acc: 0.7732\n",
      "Epoch 5/25\n",
      "87359/87359 [==============================] - 154s 2ms/step - loss: 0.6780 - acc: 0.7786\n",
      "Epoch 6/25\n",
      "87359/87359 [==============================] - 150s 2ms/step - loss: 0.6665 - acc: 0.7805\n",
      "Epoch 7/25\n",
      "87359/87359 [==============================] - 152s 2ms/step - loss: 0.6569 - acc: 0.7823\n",
      "Epoch 8/25\n",
      "87359/87359 [==============================] - 172s 2ms/step - loss: 0.6476 - acc: 0.7858\n",
      "Epoch 9/25\n",
      "87359/87359 [==============================] - 179s 2ms/step - loss: 0.6409 - acc: 0.7882\n",
      "Epoch 10/25\n",
      "87359/87359 [==============================] - 158s 2ms/step - loss: 0.6299 - acc: 0.7917\n",
      "Epoch 11/25\n",
      "87359/87359 [==============================] - 208s 2ms/step - loss: 0.6224 - acc: 0.7949\n",
      "Epoch 12/25\n",
      "87359/87359 [==============================] - 191s 2ms/step - loss: 0.6129 - acc: 0.7971\n",
      "Epoch 13/25\n",
      "87359/87359 [==============================] - 216s 2ms/step - loss: 0.6042 - acc: 0.7985\n",
      "Epoch 14/25\n",
      "87359/87359 [==============================] - 172s 2ms/step - loss: 0.5977 - acc: 0.8016\n",
      "Epoch 15/25\n",
      "87359/87359 [==============================] - 159s 2ms/step - loss: 0.5948 - acc: 0.8025\n",
      "Epoch 16/25\n",
      "87359/87359 [==============================] - 171s 2ms/step - loss: 0.5830 - acc: 0.8064\n",
      "Epoch 17/25\n",
      "87359/87359 [==============================] - 170s 2ms/step - loss: 0.5753 - acc: 0.8078\n",
      "Epoch 18/25\n",
      "87359/87359 [==============================] - 186s 2ms/step - loss: 0.5706 - acc: 0.8098\n",
      "Epoch 19/25\n",
      "87359/87359 [==============================] - 177s 2ms/step - loss: 0.5614 - acc: 0.8124\n",
      "Epoch 20/25\n",
      "87359/87359 [==============================] - 186s 2ms/step - loss: 0.5595 - acc: 0.8134\n",
      "Epoch 21/25\n",
      "87359/87359 [==============================] - 160s 2ms/step - loss: 0.5531 - acc: 0.8146\n",
      "Epoch 22/25\n",
      "87359/87359 [==============================] - 169s 2ms/step - loss: 0.5408 - acc: 0.8179\n",
      "Epoch 23/25\n",
      "87359/87359 [==============================] - 174s 2ms/step - loss: 0.5419 - acc: 0.8185\n",
      "Epoch 24/25\n",
      "87359/87359 [==============================] - 181s 2ms/step - loss: 0.5382 - acc: 0.8187\n",
      "Epoch 25/25\n",
      "87359/87359 [==============================] - 197s 2ms/step - loss: 0.5237 - acc: 0.8242\n",
      "\n",
      " Probabilistic\n",
      "\n",
      "from fairest creatures we desire increase,\n",
      "but i show for me to bubling names of brow,\n",
      "in worth in teret for thee's can lies shall excreash.\n",
      "ind' look intied mine of subsiluss give,\n",
      "look by thy heart than wastefilion feed,\n",
      "for it dort be other, where is my heart's hard,\n",
      "that summer's great hupoware eyes of your dear.\n",
      "you travel i al orn born of word,\n",
      "shall in mearing to my sud's riped pence:\n",
      "then of yourn inth on well-stife turness,\n",
      "\n",
      "\n",
      " Deterministic\n",
      "\n",
      "from fairest creatures we desire increase,\n",
      "that life no longer than which makes art of me,\n",
      "ham for my self i'll onned to are me.\n",
      "such his sigulest shall fair great new sumb,\n",
      "and that as thou alone, make the world must sing:\n",
      "making no such outward past pleast disprained,\n",
      "so doth the grous alone hours and proven,\n",
      "the fair be new false i death doth live,\n",
      "and do the expense of many a play decay,\n",
      "but in my self with thee, when what i say\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "model2.fit(X, y, epochs=25, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Probabilistic\n",
      "\n",
      "from fairest creatures we desire increase,\n",
      "thy praise twands in acquest, nor wron's:\n",
      "this to make the hang mercards to time's ears him steed,\n",
      "but be from thee, but sweet, doth note.\n",
      "nor the fairest ond did neger tooned,\n",
      "nothing thee, thy purpess are tur love doth be,\n",
      "and slay they mourned boastaft be to gole,\n",
      "for beauty's cold, with benory days are seens.\n",
      "thine own not, prowers his guinted love\n",
      "calms their powast doth life on them,\n",
      "and place may gives thee, the better perce:\n",
      "and al all men'st in thee of my fair frow,\n",
      "cowarding with your recoure within decay,\n",
      "resemblance of the dare point of sight,\n",
      "\n",
      "\n",
      " Deterministic\n",
      "\n",
      "from fairest creatures we desire increase,\n",
      "that life no longer than which makes art of me,\n",
      "ham for my self i'll onned to are me.\n",
      "such his sigulest shall fair great new sumb,\n",
      "and that as thou alone, make the world must sing:\n",
      "making no such outward past pleast disprained,\n",
      "so doth the grous alone hours and proven,\n",
      "the fair be new false i death doth live,\n",
      "and do the expense of many a play decay,\n",
      "but in my self with thee, when what i say\n",
      ",\n",
      "when it wish my love leads deared,\n",
      "and therefore art of their shary a vantare,\n",
      "some i stater kniffing thy fair shall summers be,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n Probabilistic\\n')\n",
    "p2_8 = gen_poem(model2)\n",
    "\n",
    "print('\\n Deterministic\\n')\n",
    "p2_9 = gen_det_poem(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked pretty well! Lets try increasing the window size to capture more than one line. That should theoretically yield better results, since we are covering more of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs2 = list()\n",
    "input2_size = 60\n",
    "\n",
    "for poem in poems:\n",
    "    i = input2_size\n",
    "    while i < len(poem)-1:\n",
    "        inputs2.append([poem[i-input2_size:i], poem[i]])\n",
    "        i+=1\n",
    "        \n",
    "X2 = list()\n",
    "y2 = list()\n",
    "\n",
    "for i in inputs2:\n",
    "    X2.append(batch_encode(i[0]))\n",
    "    y2.append(encode(i[1]))\n",
    "\n",
    "X2 = np.array(X2)\n",
    "y2 = np.array(y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84279, 60, 38)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_23 (LSTM)               (None, 200)               191200    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 38)                7638      \n",
      "=================================================================\n",
      "Total params: 198,838\n",
      "Trainable params: 198,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "84279/84279 [==============================] - 268s 3ms/step - loss: 2.2917 - acc: 0.3354\n",
      "Epoch 2/20\n",
      "84279/84279 [==============================] - 252s 3ms/step - loss: 1.8845 - acc: 0.4353\n",
      "Epoch 3/20\n",
      "84279/84279 [==============================] - 211s 3ms/step - loss: 1.7284 - acc: 0.4741\n",
      "Epoch 4/20\n",
      "84279/84279 [==============================] - 213s 3ms/step - loss: 1.6299 - acc: 0.4996\n",
      "Epoch 5/20\n",
      "84279/84279 [==============================] - 213s 3ms/step - loss: 1.5502 - acc: 0.5206\n",
      "Epoch 6/20\n",
      "84279/84279 [==============================] - 211s 3ms/step - loss: 1.4812 - acc: 0.5388\n",
      "Epoch 7/20\n",
      "84279/84279 [==============================] - 212s 3ms/step - loss: 1.4174 - acc: 0.5570\n",
      "Epoch 8/20\n",
      "84279/84279 [==============================] - 214s 3ms/step - loss: 1.3539 - acc: 0.5754\n",
      "Epoch 9/20\n",
      "84279/84279 [==============================] - 210s 2ms/step - loss: 1.2957 - acc: 0.5917\n",
      "Epoch 10/20\n",
      "84279/84279 [==============================] - 211s 3ms/step - loss: 1.2383 - acc: 0.6087\n",
      "Epoch 11/20\n",
      "84279/84279 [==============================] - 212s 3ms/step - loss: 1.1827 - acc: 0.6256\n",
      "Epoch 12/20\n",
      "84279/84279 [==============================] - 211s 3ms/step - loss: 1.1324 - acc: 0.6405\n",
      "Epoch 13/20\n",
      "84279/84279 [==============================] - 215s 3ms/step - loss: 1.0831 - acc: 0.6552\n",
      "Epoch 14/20\n",
      "84279/84279 [==============================] - 212s 3ms/step - loss: 1.0426 - acc: 0.6688\n",
      "Epoch 15/20\n",
      "84279/84279 [==============================] - 212s 3ms/step - loss: 0.9983 - acc: 0.6818\n",
      "Epoch 16/20\n",
      "84279/84279 [==============================] - 212s 3ms/step - loss: 0.9620 - acc: 0.6940\n",
      "Epoch 17/20\n",
      "84279/84279 [==============================] - 210s 2ms/step - loss: 0.9279 - acc: 0.7039\n",
      "Epoch 18/20\n",
      "84279/84279 [==============================] - 213s 3ms/step - loss: 0.8982 - acc: 0.7123\n",
      "Epoch 19/20\n",
      "84279/84279 [==============================] - 212s 3ms/step - loss: 0.8695 - acc: 0.7214\n",
      "Epoch 20/20\n",
      "84279/84279 [==============================] - 211s 3ms/step - loss: 0.8451 - acc: 0.7280\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_23_input to have shape (60, 38) but got array with shape (40, 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-242-1639a7593e75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mp6_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_poem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-238-fed3348e8e7b>\u001b[0m in \u001b[0;36mgen_poem\u001b[0;34m(model, seed)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-225-8f9863e3781f>\u001b[0m in \u001b[0;36mget_next\u001b[0;34m(model, line)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_next_det\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected lstm_23_input to have shape (60, 38) but got array with shape (40, 38)"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model6 = Sequential()\n",
    "\n",
    "model6.add(LSTM(200, input_shape=(X2[0].shape)))\n",
    "model6.add(Dense(encoding_size, activation='softmax'))\n",
    "\n",
    "print(model6.summary())\n",
    "\n",
    "# compile model\n",
    "model6.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "model6.fit(X2, y2, epochs=20, verbose=1)\n",
    "\n",
    "p6_1 = gen_poem(model6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also had to update the gen_poem function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fairest creatures we desire increase,\n",
      "that thereby beauty should dead rehind enerhed,\n",
      "all fair no fair with thine eye,\n",
      "and weeens so strange, but shall, with fliend in every:\n",
      "all thine all men after that herself imported,\n",
      "i know the boint, and i strate's behime,\n",
      "that in that fire me bestyest thou me, doth lide,\n",
      "when athich to pracces if your love was will,\n",
      "i patern in such riget discance of ertlime.\n",
      "i worno on these what i worthle doth spent,\n",
      "shall kind of meato's excenseans of thine eyes,\n",
      "my lime disgrars now wrak the fone, and consert,\n",
      "for if you not bemember thy self ine'en.\n",
      "for blunten to their abusesty care of more:\n",
      "shall other with this, and dead loved with shode,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p6_1 = gen_poem(model6, poems[0][:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fairest creatures we desire increase,\n",
      "that thereby beauty's fair throught to trought,\n",
      "dill me strangelal inam in every slow,\n",
      "that i am not love's frescised was still,\n",
      "to ervers seem withil thou know strange astanted:\n",
      "so lone but enjoy again was fears to end,\n",
      "and pate which reeving this, riching subscove\n",
      "tand thou my love more born morights bair,\n",
      "and arten of every hadgen vowous,\n",
      "as praise have unlaption to thy which ingrind.\n",
      "hath'r thee give live thy newleth spitts and dead.\n",
      "now bright despair thy lip, or well before,\n",
      "but your beauty mad me hirv'st those tongue,\n",
      "whose frimes in dark that never cure and dressing,\n",
      "and to be a frisies fire more friled and write?\n",
      "\n",
      "from fairest creatures we desire increase,\n",
      "that thereby beauty, and thy faults consered,\n",
      "without true lives on thy holous of your graces,\n",
      "and strongly imporatize i day doth shows,\n",
      "i love and arthory and the griefeled.\n",
      "though moce mores, excause redoot hels with thee,\n",
      "the incentummed leass me, since amised.\n",
      "unly chire do this live, for sweet becoutled feid.\n",
      "o but our inat (though my forsaded boand,\n",
      "i haw strong mand's hight past cortall me in weeds\n",
      "what strains (though they metain with clook,\n",
      "and abconn, bodo's fell, and in sine in me,\n",
      "a love a cad all my filst unly, and herfill\n",
      "dit to my sun'les mights and bainath\n",
      "no' blame me puin, my not all your geacest theme:\n",
      "\n",
      "from fairest creatures we desire increase,\n",
      "that thereby beauty's flesting i noblices:\n",
      "i am ftarring thy love may sweet besoment,\n",
      "and to thy self again, on sceech compearse,\n",
      "think exturn would give letemy snow thy flow)\n",
      "do gound it waterest by thy hand, and selfout:\n",
      "so lent my alter was blame a former might\n",
      "for stor the fair wors to show man love morn\n",
      "make but why discold hee till me hervelled\n",
      "nim you in worsty, seeking of thy beaser,\n",
      "to kind of my, not age my love can sight\n",
      "to shan they evise these poor as most.\n",
      "no fair were i will so fair wrose another,\n",
      "and my best bobnisies thy pert, he canned forey\n",
      "bore with a patity, though nem, to truth\n",
      "\n",
      "from fairest creatures we desire increase,\n",
      "that thereby beauty, that feecel a self rade,\n",
      "still so bright in thy should gave life new raie,\n",
      "doth her displazing heaven that went i sme,\n",
      "devended trispecces of not be rewinter fied,\n",
      "even so much, and they than bright on thy grape,\n",
      "even so my grain the world despised appoarrid,\n",
      "to age to show false respose to find a perases.\n",
      "than lift corrazed od clanfin dright to love,\n",
      "do i enewe a fumbs of thy parts besterment,\n",
      "lives out the subser madaty will show gove,\n",
      "what in death's fors, my self, and reason bo?\n",
      "against to till the mointy of all mine\n",
      "eye to seems that not so are one known peres\n",
      "bateivith thou groul's fors, and to those care,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p6_2 = gen_poem(model6, poems[0][:60])\n",
    "p6_3 = gen_poem(model6, poems[0][:60])\n",
    "p6_4 = gen_poem(model6, poems[0][:60])\n",
    "p6_5 = gen_poem(model6, poems[0][:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the poems with a window size of 60 looked very good. If I did not speak english then you could absolutely convince me that these are good poems. This is mainly because it looks like english, even if it is not. Finally lets try decreasing the window size to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs3 = list()\n",
    "input3_size = 20\n",
    "\n",
    "for poem in poems:\n",
    "    i = input3_size\n",
    "    while i < len(poem)-1:\n",
    "        inputs3.append([poem[i-input3_size:i], poem[i]])\n",
    "        i+=1\n",
    "        \n",
    "X3 = list()\n",
    "y3 = list()\n",
    "\n",
    "for i in inputs3:\n",
    "    X3.append(batch_encode(i[0]))\n",
    "    y3.append(encode(i[1]))\n",
    "\n",
    "X3 = np.array(X3)\n",
    "y3 = np.array(y3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90439, 20, 38)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_26 (LSTM)               (None, 200)               191200    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 38)                7638      \n",
      "=================================================================\n",
      "Total params: 198,838\n",
      "Trainable params: 198,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "90439/90439 [==============================] - 102s 1ms/step - loss: 2.2703 - acc: 0.3428\n",
      "Epoch 2/5\n",
      "90439/90439 [==============================] - 102s 1ms/step - loss: 1.8838 - acc: 0.4349\n",
      "Epoch 3/5\n",
      "90439/90439 [==============================] - 100s 1ms/step - loss: 1.7378 - acc: 0.4695\n",
      "Epoch 4/5\n",
      "90439/90439 [==============================] - 102s 1ms/step - loss: 1.6396 - acc: 0.4946\n",
      "Epoch 5/5\n",
      "90439/90439 [==============================] - 126s 1ms/step - loss: 1.5600 - acc: 0.5160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b1ed3748>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model7 = Sequential()\n",
    "\n",
    "model7.add(LSTM(200, input_shape=(X3[0].shape)))\n",
    "model7.add(Dense(encoding_size, activation='softmax'))\n",
    "\n",
    "print(model7.summary())\n",
    "\n",
    "# compile model\n",
    "model7.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "model7.fit(X3, y3, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fairest creature,\n",
      "that spareth mish.\n",
      "so dott datiand a jostens fortush of thy hast thou lov'st greanst post of prace,\n",
      "keal doth woild wish a somening?\n",
      "san love bald thing to the tim,\n",
      "to be strengering owher, but the with can to praise,\n",
      "beauteds freethous live your still out is his mest abuin:\n",
      "nor that for the worth's with one.\n",
      "that if shalt sick my gove,\n",
      "wherey shink nosed so that a fore,\n",
      "so muth recention not tone.\n",
      "frilth notere sluble thee, thou white's ithen't trunged,\n",
      "thon do alass as never,\n",
      "and bearuy, a bestered doth with ander difthe here,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p7_1 = gen_poem(model7, poems[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That did not work quite as well. This makes sense since we are covering less of the text, meaning there is less info to base future decisions off of. Lets try training a bit more, to see if we can increase the quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "90439/90439 [==============================] - 134s 1ms/step - loss: 1.4908 - acc: 0.5341\n",
      "Epoch 2/5\n",
      "90439/90439 [==============================] - 152s 2ms/step - loss: 1.4274 - acc: 0.5526\n",
      "Epoch 3/5\n",
      "90439/90439 [==============================] - 123s 1ms/step - loss: 1.3654 - acc: 0.5697\n",
      "Epoch 4/5\n",
      "90439/90439 [==============================] - 236s 3ms/step - loss: 1.3033 - acc: 0.5870\n",
      "Epoch 5/5\n",
      "90439/90439 [==============================] - 102s 1ms/step - loss: 1.2414 - acc: 0.6045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b5037e48>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model7.fit(X3, y3, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fairest creatured deeds,\n",
      "my viciles the piop in one, and you is do not tife,\n",
      "the rate my soul, when of worth, with try speace mays what then winw.\n",
      "o wo thy breath (d-live manss of i awoung thy beauty to say thy mimituen offect,\n",
      "and sadce i astreatl,\n",
      "hem and trues, or fur to ungrespe to ripery with pay a vering :\n",
      "be,\n",
      "but an your part be spown,\n",
      "we to whete as thy vorst is suswer of your tootuqee near.\n",
      "and all thin wornd me trilms a disfarxed things thy much of night\n",
      "wand for my trues while wat a men,\n",
      "in new he all my self tebbow,\n",
      "which winturny beause of sweetcepget with subjectles in her writely of arthroses,\n",
      "and you no'e must lecg, then 'ericted baded flowers,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p7_2 = gen_poem(model7, poems[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could not. In fact, the poem above actually looks worse than the first model 7 poem. Both are far overshadowed by model 6, which will most likely be the final one we go with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fairest creatures we desire increase,\n",
      "that thereby beauty's songes' colfined beased,\n",
      "but do not so long, and froshan and from thought\n",
      "to wake the minded stain art so gurled lies,\n",
      "lest soor with poor, and give make the rest?\n",
      "no kif seems foul adour abtiquilainies\n",
      "passoush one, ever so, no eat a tomber die,\n",
      "thou canst to see as any, me fort so oft\n",
      "as their is ton race, retths of sovereign pain.\n",
      "as thought inlease him all divined pace weed,\n",
      "thy proud ear kind, which to show wate in thee.\n",
      "th' barths even as fot than wetersh store,\n",
      "haw dupy praise i dide not in worth thy seek,\n",
      "but you are your slame to make the sugh my migd.\n",
      "hampies to spain to make to wook being mane.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p6_6 = gen_poem(model6, poems[0][:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_32 (LSTM)               (None, 250)               289000    \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 38)                9538      \n",
      "=================================================================\n",
      "Total params: 298,538\n",
      "Trainable params: 298,538\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "84279/84279 [==============================] - 349s 4ms/step - loss: 2.2972 - acc: 0.3349\n",
      "Epoch 2/2\n",
      "84279/84279 [==============================] - 317s 4ms/step - loss: 1.8837 - acc: 0.4332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c11e4b00>"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model8 = Sequential()\n",
    "model8.add(LSTM(250, input_shape=(X2[0].shape)))\n",
    "model8.add(Dense(encoding_size, activation='softmax'))\n",
    "           \n",
    "model8.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model8.summary())\n",
    "\n",
    "# fit model\n",
    "model8.fit(X2, y2, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried increasing the LSTM size, but it appears that 200 works best. As such, we will finish part 6 here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
